-------Count of Null, None, NaN of All DataFrame Columns----------------------------
from pyspark.sql.functions import col,isnan, when, count
from pyspark.sql import SparkSession
from pyspark.sql.functions import col,isnan, when, count
#import pandas as pd 
# Create SparkSession 
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("SparkByExamples.com") \
      .getOrCreate() 
      
print(spark)  
df=spark.read.option("header",True).csv("C:/Users/prashantm5.MUMBAI1/Downloads/userdata1.csv")
#this is called comprehensive list 
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]
   ).show()

+----+-----+------+
|name|state|number|
+----+-----+------+
|   0|    1|     3|
+----+-----+------+

----------- rename all column name with prefix ----------------------
import pyspark.sql.functions as F
def rename(emp_df):
    for x in emp_df.columns:
        emp_df = emp_df.withColumnRenamed(x, 'col_'+x)
    return emp_df

new_df = rename(emp_df)
new_df.display()
--------------------------------------------------------------------------------
from pyspark import SparkContext, SparkConf
from pyspark.sql import HiveContext
 
#Main module to execute spark code
if __name__ == '__main__':
    conf = SparkConf() #Declare spark conf variable\
    conf.setAppName("Read-and-write-data-to-Hive-table-spark")
    sc = SparkContext.getOrCreate(conf=conf)
 
    #Instantiate hive context class to get access to the hive sql method  
    hc = HiveContext(sc)
 
    #Read hive table in spark using .sql method of hivecontext class
    df = hc.sql("select * from default.hive_read_write_demo")
 
    #Display the spark dataframe values using show method
    df.show(10, truncate = False)
-----------------------------------------------------------------------
# word count----------
rdd = sc.textFile('path')
words = rdd.flatmap(lambda line: line.split(' '))
wordcount = words.map(word: (word,1)).reduceByKey(lambda a,b:a+b)
wordcount.saveAsTextFile('path')

--------------------------------------------------------------------------------
Input dataframe----
c1 c2
1 a|b
2 a|e|d
c f
output dataframe----
c1 c2
1 a
1 b
2 a
2 e
2 d
c f
-------

df = df.selelct(c1, explode(constr(c2)))

def constr(s):
	val = s.split('|')
	return val
--------------------------------------------------------------------------	
dfPersist = df.persist(StorageLevel.MEMORY_ONLY)
dfPersist.show(false)
dfPersist = dfPersist.unpersist()
--------------------------------------------------------------------------
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr
#Create spark session
data = [("Banana",1000,"USA"), ("Carrots",1500,"USA"), ("Beans",1600,"USA"), \
      ("Orange",2000,"USA"),("Orange",2000,"USA"),("Banana",400,"China"), \
      ("Carrots",1200,"China"),("Beans",1500,"China"),("Orange",4000,"China"), \
      ("Banana",2000,"Canada"),("Carrots",2000,"Canada"),("Beans",2000,"Mexico")]

columns= ["Product","Amount","Country"]
df = spark.createDataFrame(data = data, schema = columns)
df.printSchema()
df.show(truncate=False)

-------------------------------------------------------------------------------------
------------- PIVOT example --------------

pivotDF = df.groupBy("Product").pivot("Country").sum("Amount")
pivotDF.printSchema()
pivotDF.show(truncate=False)

----------OR with prformance improvement------------

countries = ["USA","China","Canada","Mexico"]
pivotDF = df.groupBy("Product").pivot("Country", countries).sum("Amount")
pivotDF.show(truncate=False)

------------- Or ----------------------------

pivotDF = df.groupBy("Product","Country") \
      .sum("Amount") \
      .groupBy("Product") \
      .pivot("Country") \
      .sum("sum(Amount)") \
pivotDF.show(truncate=False)
--------------------------------------------------------------------------
------------- Unpivot ----------------

from pyspark.sql.functions import expr
unpivotExpr = "stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)"
unPivotDF = pivotDF.select("Product", expr(unpivotExpr)) \
    .where("Total is not null")
unPivotDF.show(truncate=False)
unPivotDF.show()

---------- Pivot & Unpivot example ---------------

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [("Banana",1000,"USA"), ("Carrots",1500,"USA"), ("Beans",1600,"USA"), \
      ("Orange",2000,"USA"),("Orange",2000,"USA"),("Banana",400,"China"), \
      ("Carrots",1200,"China"),("Beans",1500,"China"),("Orange",4000,"China"), \
      ("Banana",2000,"Canada"),("Carrots",2000,"Canada"),("Beans",2000,"Mexico")]

columns= ["Product","Amount","Country"]
df = spark.createDataFrame(data = data, schema = columns)
df.printSchema()
df.show(truncate=False)

pivotDF = df.groupBy("Product").pivot("Country").sum("Amount")
pivotDF.printSchema()
pivotDF.show(truncate=False)

pivotDF = df.groupBy("Product","Country") \
      .sum("Amount") \
      .groupBy("Product") \
      .pivot("Country") \
      .sum("sum(Amount)")
pivotDF.printSchema()
pivotDF.show(truncate=False)

""" unpivot """
unpivotExpr = "stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)"
unPivotDF = pivotDF.select("Product", expr(unpivotExpr)) \
    .where("Total is not null")
unPivotDF.show(truncate=False)

-------------------------------------------------------------------------------------------------------------
i have below two dataframes
dataframe1 with below data

col1
1
2
3

dataframe2 with below data 

col2
3
4
5

i want output to be below dataframe with 3rd column is addition of 1st and 2nd column 

col1 col2 col3 
1     3    4
2     4     6
3     5    8

ans :-- Use the row_number function to add a sequential ID to each dataframe and join them based on this ID
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

# Initialize a Spark session
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()

# Create the first dataframe
data1 = [(1,), (2,), (3,)]
df1 = spark.createDataFrame(data1, ["col1"])

# Create the second dataframe
data2 = [(3,), (4,), (5,)]
df2 = spark.createDataFrame(data2, ["col2"])

# Add a row number to each dataframe
window_spec = Window.orderBy("col1")
df1 = df1.withColumn("row_id", row_number().over(window_spec))

window_spec2 = Window.orderBy("col2")
df2 = df2.withColumn("row_id", row_number().over(window_spec2))

# Join the dataframes on the row_id
df_combined = df1.join(df2, on="row_id").drop("row_id")

# Add the third column which is the sum of the first two columns
df_result = df_combined.withColumn("col3", col("col1") + col("col2"))

# Show the result
df_result.show()



