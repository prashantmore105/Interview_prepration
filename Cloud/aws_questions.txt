1. s3 life cycle management
  high durablity 99.999999 % 
  availability  99.99% that measn it wont be available for 53 minutes a year
Different types of S3 Storages available:
  1. Amazon S3 Standard — General Purpose:
     This is the default storage and is used frequently.
      used for frequently used data 
   Eg: Big Data Analytics, mobile and gaming app, content distributions

  2. S3 Standard — IA (Infrequent Access):
     -->This type of storage should be used when you are not using the data that often. 
	    When you use them, you need them rapidly.
      → Cost is optimized from the general plan.
      -->lower cost compared to S3 but  retrival fees is present
    Eg: Datastore for the disaster recovery backups.
  3. S3 One Zone IA (Infrequent Access):

	→ This one is cheaper than the previous S3 IA storage (< 20%).
	The difference is it is stored on only one A-Z (i.e. availability zone).
	99.5  avaliablity 
	low cost 
	low latency
	low throughput
  
  4. S3 Intelligent-Tiering:

	→ If you do not know what to choose between General and IA plan storage, this one is for you.

	→ The less-used file will be moved to IA mode and others used in Standard storage mode.
	 availability 99.9 
	 same performance and throughput as s3 standard
	 cost optimized by automatically moving objects brtween two access tiers based on chnage in access patterns 
      frequent and infrequent access

   5. Amazon Glaciers & Amazon Glacier Deep Archive:

	→ This storage is used when some data needs to be frozen for a certain period of time.

	→ Data can be retrieved quickly as listed in the fees plans below.

	→ They are cheap in price. Amazon Glacier Deep Archive is the cheapest.
    -- low cose storage ment for archieving and backup


https://aws.amazon.com/s3/storage-classes/
https://aws.plainenglish.io/aws-s3-different-types-of-storage-types-available-in-s3-3550e0b87580

2. VPC :- 
--With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual 
network that you've defined. This virtual network closely resembles a traditional network that you'd operate in 
your own data center, with the benefits of using the scalable infrastructure of AWS.

--The following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, 
EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.
Virtual private clouds (VPC)
A VPC is a virtual network that closely resembles a traditional network that you'd operate in your own data center. 
After you create a VPC, you can add subnets.
Subnets
A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. 
After you add subnets, you can deploy AWS resources in your VPC.


3. why EMR not used
 Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, 
 such as Apache Hadoop and Apache Spark , on AWS to process and analyze vast amounts of data.
 EC2 vs EMR
	EC2 (Elastic Compute Cloud) are generic Linux or Windows servers that you can use to run anything you want. However if you want something like a distributed Hadoop cluster, or an RDBMS cluster, you will end up spending a lot of time configuring that.

	EMR (Elastic Map Reduce) is, as the name implies, specifically configured for handling map reduce jobs via tools like Hadoop and Spark. It is pre-configured and ready to start processing your map reduce jobs.

	EMR is just a service built on top of EC2 to make things like distributed map reduce jobs easier to perform. It takes away all the pain of setting up a distributed compute cluster yourself. Similar to how RDS is a managed database service built on top of EC2 that manages things like backups, read replicas and disaster recovery for you.
 
4. CLoud formation template 
https://spacelift.io/blog/terraform-vs-cloudformation
5. Oracastraction tool used
  --aws glue workflow
      --event bridge 
	   --lambda to pass parameters 
  --Airflow
6. Data migration 

8. GLUE VS EC2 instance
9. Redshift 
--Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, 
   operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver 
   the best price performance at any scale.
-- data is stored as columanar storage
-- supports sql tool to retival of data 
--sql queries and stored procedure works 
10.Step function vs glue workflow