1. s3 life cycle management
  high durablity 99.999999 % 
  availability  99.99% that measn it wont be available for 53 minutes a year
Different types of S3 Storages available:
  1. Amazon S3 Standard — General Purpose:
     This is the default storage and is used frequently.
      used for frequently used data 
   Eg: Big Data Analytics, mobile and gaming app, content distributions

  2. S3 Standard — IA (Infrequent Access):
     -->This type of storage should be used when you are not using the data that often. 
	    When you use them, you need them rapidly.
      → Cost is optimized from the general plan.
      -->lower cost compared to S3 but  retrival fees is present
    Eg: Datastore for the disaster recovery backups.
  3. S3 One Zone IA (Infrequent Access):

	→ This one is cheaper than the previous S3 IA storage (< 20%).
	The difference is it is stored on only one A-Z (i.e. availability zone).
	99.5  avaliablity 
	low cost 
	low latency
	low throughput
  
  4. S3 Intelligent-Tiering:

	→ If you do not know what to choose between General and IA plan storage, this one is for you.

	→ The less-used file will be moved to IA mode and others used in Standard storage mode.
	 availability 99.9 
	 same performance and throughput as s3 standard
	 cost optimized by automatically moving objects brtween two access tiers based on chnage in access patterns 
      frequent and infrequent access

   5. Amazon Glaciers & Amazon Glacier Deep Archive:

	→ This storage is used when some data needs to be frozen for a certain period of time.

	→ Data can be retrieved quickly as listed in the fees plans below.

	→ They are cheap in price. Amazon Glacier Deep Archive is the cheapest.
    -- low cose storage ment for archieving and backup


https://aws.amazon.com/s3/storage-classes/
https://aws.plainenglish.io/aws-s3-different-types-of-storage-types-available-in-s3-3550e0b87580

2. VPC :- 
--With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual 
network that you've defined. This virtual network closely resembles a traditional network that you'd operate in 
your own data center, with the benefits of using the scalable infrastructure of AWS.

--The following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, 
EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.
Virtual private clouds (VPC)
A VPC is a virtual network that closely resembles a traditional network that you'd operate in your own data center. 
After you create a VPC, you can add subnets.
Subnets
A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. 
After you add subnets, you can deploy AWS resources in your VPC.


3. why EMR not used
 Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, 
 such as Apache Hadoop and Apache Spark , on AWS to process and analyze vast amounts of data.
 EC2 vs EMR
	EC2 (Elastic Compute Cloud) are generic Linux or Windows servers that you can use to run anything you want. However if you want something like a distributed Hadoop cluster, or an RDBMS cluster, you will end up spending a lot of time configuring that.

	EMR (Elastic Map Reduce) is, as the name implies, specifically configured for handling map reduce jobs via tools like Hadoop and Spark. It is pre-configured and ready to start processing your map reduce jobs.

	EMR is just a service built on top of EC2 to make things like distributed map reduce jobs easier to perform. It takes away all the pain of setting up a distributed compute cluster yourself. Similar to how RDS is a managed database service built on top of EC2 that manages things like backups, read replicas and disaster recovery for you.
 
4. CLoud formation template 
https://spacelift.io/blog/terraform-vs-cloudformation
5. Oracastraction tool used
  --aws glue workflow
      --event bridge 
	   --lambda to pass parameters 
  --Airflow
6. Data migration 

8. GLUE VS EMR
Use case , requirements and preferences 
focus was on the ETL Tasks ,and we prefered the serverless and manged soluction  

1. serverless manged ETL .dont hav eto provison and mange the underlying infrastructure.
2. Pay as go model with Pay for resources as used for ETL jobs 
   leads to cost saving because you are paying for resources that are consumed .
3. integration with other aws services .
3. scalability :-scales autimatically based on size of data and transformation   
   
EMR
 1. Manage the infra and do the configuration 
2. variable cost and depenf]ds on the resources provisined  
3. scalalbility :- more cpontrol over cluster size and configurations

IF focous in on ETL taksks and preference is a sverless and mangaged soluction glu is is the right fit .

more fexibility on big data processing environment using various computing framework thn emr
9. Redshift 
--Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, 
   operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver 
   the best price performance at any scale.
-- data is stored as columanar storage
-- supports sql tool to retival of data 
--sql queries and stored procedure works 

10.Step function vs glue workflow

11. spark datarame vs glue dynamic frame
--A DynamicFrame is similar to a DataFrame, except that each record is self-describing, 
  so no schema is required initially. Instead, AWS Glue computes a schema on-the-fly
-- You can convert DynamicFrames to and from DataFrames after you resolve any schema inconsistencies.
-- main feature is job bookmark and read writr ablity and integration with aws 
https://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/


12.
aws
---------
>S3 storage capcity  5 TB
>S3 by default files can be store 1000, aws support but in total 5 TB
>Lambda 15 duration after that timeout> small logics
>Lambda layer to  add libraries or dependent packages
>event bridge for scheduling
>cross account> trust relation need be updated
>glue default 48 hours after timeout
>crwaler to scan s3 data and table in catalog
>clssifiers data format identification
>athena we can create table without any s3 location in athena all are external tables
>workflow is for define logical steps of the jobs >crawler, glue job
>through trigger we can add crawler or glue job
>emr bigdata ware house platform in emr 
>redhshit is data warehouse >regluar>external>temporary tables> temporary with external tables  

13 . glue read and write df code
--read
df_all = glueContext.create_dynamic_frame.from_catalog(database=raw_catalogue_db_name,
                                                       table_name=table_with_source,
                                                       push_down_predicate=f"(year >= '{predicate_year}')",
                                                       transformation_ctx="datasource0").toDF() 
													   
--write
datasink = glueContext.write_dynamic_frame.from_options(frame=datasource0,
                                                    connection_type="s3",
                                                     connection_options={"path": path,
                                                      "partitionKeys": [
                                                       "year", "month",
                                                        "day"]},
                                                        format="parquet",
                                                         transformation_ctx="datasink")


14. lambda code
import os,boto3,logger													   \
region=os.region["AWS region"]

def lambda_handler(event)
	glue=boto3.client(servicename="glue" ,region =awsregion )
	
	wf_name=str(event["wf_name"])
	is_phi=
	job_id=
	
	glue.update_workflow(name ="wf_nme" ,runpro[perties="" )
	responce=glue.start_workflow("name")
	logger.info("started successfully")

--------------------------------------------------------------------------------------------
15. can aws lambda function runs outside the vpc as well ?
Yes, AWS Lambda functions can run both inside and outside a Virtual Private Cloud (VPC). When you create a Lambda function, you have the option to configure it to run either inside or outside a VPC, depending on your requirements.

Outside VPC (default): By default, Lambda functions run outside a VPC. When a Lambda function runs outside a VPC, it does not have access to resources within the VPC, such as EC2 instances, RDS databases, or ElastiCache clusters. However, it can still access internet resources and AWS services that are accessible via public endpoints.

Inside VPC: You can configure a Lambda function to run inside a VPC, which allows it to access resources within the VPC, including private subnets and resources with private IP addresses. When you run a Lambda function inside a VPC, it is provisioned with an elastic network interface (ENI) in each subnet specified, enabling it to communicate with resources within the VPC.

Running Lambda functions inside a VPC is often used when the function needs to access private resources securely, without exposing them to the public internet. However, it's essential to consider that running Lambda functions inside a VPC may introduce additional cold start latency and networking complexities, so it should be done judiciously based on the specific requirements of your application.
In summary, AWS Lambda provides the flexibility to run functions both inside and outside a VPC, allowing you to choose the configuration that best meets your security, networking, and performance needs.
--------------------------------------------------------------------------------------------------------------
16 is s3 bucket inside or outside the vpc ?
Amazon S3 (Simple Storage Service) buckets are considered outside of Virtual Private Clouds (VPCs) in AWS.

S3 is a fully managed object storage service that is accessible over the public internet. When you create an S3 bucket, it exists outside of any VPC. S3 buckets are globally accessible and do not reside within the network boundaries of a VPC.

While you can access S3 buckets from resources inside a VPC, such as EC2 instances or Lambda functions running inside a VPC, the S3 service itself is not part of the VPC infrastructure.

When accessing S3 from resources within a VPC, you can control access using IAM policies, bucket policies, and VPC endpoints to ensure secure and private communication between your VPC resources and S3 without needing to expose your data to the public internet.

----------------------------------------------------------------------------------------------------------





	
	
