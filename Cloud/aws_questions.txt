1. s3 life cycle management
  high durablity 99.999999 % 
  availability  99.99% that measn it wont be available for 53 minutes a year
Different types of S3 Storages available:
  1. Amazon S3 Standard — General Purpose:
     This is the default storage and is used frequently.
      used for frequently used data 
   Eg: Big Data Analytics, mobile and gaming app, content distributions

  2. S3 Standard — IA (Infrequent Access):
     -->This type of storage should be used when you are not using the data that often. 
	    When you use them, you need them rapidly.
      → Cost is optimized from the general plan.
      -->lower cost compared to S3 but  retrival fees is present
    Eg: Datastore for the disaster recovery backups.
  3. S3 One Zone IA (Infrequent Access):

	→ This one is cheaper than the previous S3 IA storage (< 20%).
	The difference is it is stored on only one A-Z (i.e. availability zone).
	99.5  avaliablity 
	low cost 
	low latency
	low throughput
  
  4. S3 Intelligent-Tiering:

	→ If you do not know what to choose between General and IA plan storage, this one is for you.

	→ The less-used file will be moved to IA mode and others used in Standard storage mode.
	 availability 99.9 
	 same performance and throughput as s3 standard
	 cost optimized by automatically moving objects brtween two access tiers based on chnage in access patterns 
      frequent and infrequent access

   5. Amazon Glaciers & Amazon Glacier Deep Archive:

	→ This storage is used when some data needs to be frozen for a certain period of time.

	→ Data can be retrieved quickly as listed in the fees plans below.

	→ They are cheap in price. Amazon Glacier Deep Archive is the cheapest.
    -- low cose storage ment for archieving and backup


https://aws.amazon.com/s3/storage-classes/
https://aws.plainenglish.io/aws-s3-different-types-of-storage-types-available-in-s3-3550e0b87580

2. VPC :- 
--With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual 
network that you've defined. This virtual network closely resembles a traditional network that you'd operate in 
your own data center, with the benefits of using the scalable infrastructure of AWS.

--The following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, 
EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.
Virtual private clouds (VPC)
A VPC is a virtual network that closely resembles a traditional network that you'd operate in your own data center. 
After you create a VPC, you can add subnets.
Subnets
A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. 
After you add subnets, you can deploy AWS resources in your VPC.


3. why EMR not used
 Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, 
 such as Apache Hadoop and Apache Spark , on AWS to process and analyze vast amounts of data.
 EC2 vs EMR
	EC2 (Elastic Compute Cloud) are generic Linux or Windows servers that you can use to run anything you want. However if you want something like a distributed Hadoop cluster, or an RDBMS cluster, you will end up spending a lot of time configuring that.

	EMR (Elastic Map Reduce) is, as the name implies, specifically configured for handling map reduce jobs via tools like Hadoop and Spark. It is pre-configured and ready to start processing your map reduce jobs.

	EMR is just a service built on top of EC2 to make things like distributed map reduce jobs easier to perform. It takes away all the pain of setting up a distributed compute cluster yourself. Similar to how RDS is a managed database service built on top of EC2 that manages things like backups, read replicas and disaster recovery for you.
 
4. CLoud formation template 
https://spacelift.io/blog/terraform-vs-cloudformation
5. Oracastraction tool used
  --aws glue workflow
      --event bridge 
	   --lambda to pass parameters 
  --Airflow
6. Data migration 

8. GLUE VS EMR
Use case , requirements and preferences 
focus was on the ETL Tasks ,and we prefered the serverless and manged soluction  

1. serverless manged ETL .dont hav eto provison and mange the underlying infrastructure.
2. Pay as go model with Pay for resources as used for ETL jobs 
   leads to cost saving because you are paying for resources that are consumed .
3. integration with other aws services .
3. scalability :-scales autimatically based on size of data and transformation   
   
EMR
 1. Manage the infra and do the configuration 
2. variable cost and depenf]ds on the resources provisined  
3. scalalbility :- more cpontrol over cluster size and configurations

IF focous in on ETL taksks and preference is a sverless and mangaged soluction glu is is the right fit .

more fexibility on big data processing environment using various computing framework thn emr
9. Redshift 
--Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, 
   operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver 
   the best price performance at any scale.
-- data is stored as columanar storage
-- supports sql tool to retival of data 
--sql queries and stored procedure works 

10.Step function vs glue workflow

11. spark datarame vs glue dynamic frame
--A DynamicFrame is similar to a DataFrame, except that each record is self-describing, 
  so no schema is required initially. Instead, AWS Glue computes a schema on-the-fly
-- You can convert DynamicFrames to and from DataFrames after you resolve any schema inconsistencies.
-- main feature is job bookmark and read writr ablity and integration with aws 
https://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/
Spark DataFrame
Spark DataFrame is a distributed collection of data organized into named columns, conceptually similar to a table in a relational database or a data frame in R/Python. It is a core abstraction in Apache Spark.

Key Features:
Schema: Spark DataFrames have a well-defined schema, which means the structure of the data (names, types of columns) is known beforehand.
APIs: Provides rich APIs in Scala, Java, Python, and R for various data operations.
Performance: Optimized through Catalyst Optimizer and Tungsten Execution Engine, providing efficient querying and computation.
Interoperability: Easily integrates with other Spark components like Spark SQL, MLlib, GraphX, and more.
Transformations and Actions: Supports a wide range of transformations (e.g., map, filter, groupBy) and actions (e.g., collect, saveAsTable).

AWS Glue DynamicFrame
AWS Glue DynamicFrame is a distributed collection of data that supports a nested data structure and provides advanced features designed specifically for ETL (Extract, Transform, Load) processes. It is part of AWS Glue, a fully managed ETL service.

Key Features:
Schema Flexibility: Handles semi-structured data with a flexible schema. DynamicFrames do not require a fixed schema and can handle schema changes (e.g., missing or new columns) gracefully.
AWS Glue Integration: Seamlessly integrates with AWS Glue's Data Catalog, job scheduling, and other ETL functionalities.
Built-in Transformations: Offers specialized ETL transformations such as applyMapping, resolveChoice, and relationalize that simplify common ETL tasks.
Error Handling: Provides features to handle errors and missing data without failing the entire job.
Interoperability: Can be converted to and from Spark DataFrames, allowing you to leverage Spark's powerful processing capabilities when needed.



12.
aws
---------
>S3 storage capcity  5 TB
>S3 by default files can be store 1000, aws support but in total 5 TB
>Lambda 15 mins duration after that timeout > used to handle small logics (parameter passing , launching glue job , trigger jobs based on file arrival on s3 bucket)
>Lambda layer to  add libraries or dependent packages
>event bridge for scheduling
>cross account replication > trust relation need be updated
>glue default 48 hours after timeout
>crwaler to scan s3 data and create /update table in catalog
>Classifiers are used to define data format identification
AWS Glue classifiers are used to determine the schema of your data sources during the ETL process. When you define a data source for an AWS Glue job, classifiers help Glue understand the format and structure of the data, so it can properly catalog and transform the data.

>athena we can not create table without any s3 location in athena all are external tables ,no internal tables.
>workflow is for define logical steps of the jobs >crawler, glue job
>through trigger we can add crawler or glue job
>emr bigdata ware house platform in emr 
>redhshit is data warehouse >regluar>external>temporary tables> temporary with external tables  

13 . glue read and write df code
--read
datasource0 = glueContext.create_dynamic_frame.from_catalog(database=raw_catalogue_db_name,
                                                       table_name=table_with_source,
                                                       push_down_predicate=f"(year >= '{predicate_year}')",
                                                       transformation_ctx="datasource0").toDF() 
													   
--write
datasink = glueContext.write_dynamic_frame.from_options(frame=datasource0,
                                                    connection_type="s3",
                                                     connection_options={"path": path,
                                                      "partitionKeys": [
                                                       "year", "month",
                                                        "day"]},
                                                        format="parquet",
                                                         transformation_ctx="datasink")


14. lambda code
import os,boto3,logger													   \
region=os.region["AWS region"]

def lambda_handler(event)
	glue=boto3.client(servicename="glue" ,region =awsregion )
	
	wf_name=str(event["wf_name"])
	is_phi=
	job_id=
	
	glue.update_workflow(name ="wf_nme" ,runpro[perties="" )
	responce=glue.start_workflow("name")
	logger.info("started successfully")

--------------------------------------------------------------------------------------------
15. can aws lambda function runs outside the vpc as well ?
Yes, AWS Lambda functions can run both inside and outside a Virtual Private Cloud (VPC). When you create a Lambda function, you have the option to configure it to run either inside or outside a VPC, depending on your requirements.

Outside VPC (default): By default, Lambda functions run outside a VPC. When a Lambda function runs outside a VPC, it does not have access to resources within the VPC, such as EC2 instances, RDS databases, or ElastiCache clusters. However, it can still access internet resources and AWS services that are accessible via public endpoints.

Inside VPC: You can configure a Lambda function to run inside a VPC, which allows it to access resources within the VPC, including private subnets and resources with private IP addresses. When you run a Lambda function inside a VPC, it is provisioned with an elastic network interface (ENI) in each subnet specified, enabling it to communicate with resources within the VPC.

Running Lambda functions inside a VPC is often used when the function needs to access private resources securely, without exposing them to the public internet. However, it's essential to consider that running Lambda functions inside a VPC may introduce additional cold start latency and networking complexities, so it should be done judiciously based on the specific requirements of your application.
In summary, AWS Lambda provides the flexibility to run functions both inside and outside a VPC, allowing you to choose the configuration that best meets your security, networking, and performance needs.
--------------------------------------------------------------------------------------------------------------
16 is s3 bucket inside or outside the vpc ?
Amazon S3 (Simple Storage Service) buckets are considered outside of Virtual Private Clouds (VPCs) in AWS.

S3 is a fully managed object storage service that is accessible over the public internet. When you create an S3 bucket, it exists outside of any VPC. S3 buckets are globally accessible and do not reside within the network boundaries of a VPC.

While you can access S3 buckets from resources inside a VPC, such as EC2 instances or Lambda functions running inside a VPC, the S3 service itself is not part of the VPC infrastructure.

When accessing S3 from resources within a VPC, you can control access using IAM policies, bucket policies, and VPC endpoints to ensure secure and private communication between your VPC resources and S3 without needing to expose your data to the public internet.

----------------------------------------------------------------------------------------------------------
17. if s3 is outside the vpc then how other services access the data in s3 and is data stored in s3 safe and is it exposed to internet for everyone ?
Although Amazon S3 (Simple Storage Service) buckets are outside Virtual Private Clouds (VPCs), other AWS services and resources can access data stored in S3 securely without exposing it to the public internet. Here's how it works:

AWS Services Integration: AWS provides built-in integration between S3 and other AWS services. For example, services like AWS Lambda, Amazon EC2, Amazon EMR, Amazon Redshift, and AWS Glue can directly access data stored in S3 without needing to go through the public internet. This integration allows these services to securely read and write data to S3 buckets from within your VPC.

IAM Policies: Access to data stored in S3 is controlled using IAM (Identity and Access Management) policies. You can define IAM policies to grant permissions to specific AWS services, IAM users, roles, or even specific IP addresses to access S3 buckets and objects. By configuring IAM policies, you can restrict access to S3 data to only authorized entities, ensuring that your data remains secure.

VPC Endpoints: AWS offers VPC endpoints, which allow resources within a VPC to securely access AWS services like S3 without using public IPs or traversing the public internet. You can create VPC endpoints for S3 (called Gateway Endpoints for S3) to enable private connectivity between your VPC and S3 without exposing your data to the public internet.

Bucket Policies: In addition to IAM policies, you can also use bucket policies to control access to S3 buckets and objects. Bucket policies are JSON-based policies attached directly to S3 buckets and allow you to define fine-grained access controls based on various conditions, including the source IP address, VPC endpoint, or IAM identity.

Regarding the safety of data stored in S3 and its exposure to the internet:

Data Safety: Amazon S3 is designed to provide 99.999999999% (11 nines) durability of objects stored in the service. It replicates data across multiple availability zones within a region to ensure redundancy and durability. Additionally, S3 offers features like versioning, encryption (both in transit and at rest), and access logging to help protect and monitor your data.

Exposure to the Internet: By default, S3 buckets and objects are private, and access is denied to everyone except the bucket owner. However, it's essential to be cautious when configuring bucket policies and IAM policies to ensure that you don't inadvertently expose your data to the public internet. With proper configuration and security measures, you can keep your data stored in S3 safe and secure from unauthorized access.

----------------------------------------------------------------------------------------------------------------------





	
	
