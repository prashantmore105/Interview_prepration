1. Explain the work you have done in the previous project , regarding AWS and Spark. questions around the project
2.  Design a system with RDS system as input get the data in cloud environment and perform DQ and transformation and get the data ready for analytics
   and provide the appropriate security.
3. what is VPC , subnet and security group in detail . ans in Cloud/aws_questions.txt
4. Is Lambda inside the VPC or not ?  ans in Cloud/aws_questions.txt
5. Can lambda would work outside the VPC ? detailed explanation on how lambda works?-  ans in Cloud/aws_questions.txt
6. Is s3 bucket inside the VPC ? ans in Cloud/aws_questions.txt
7. If s3 is global then how data is protected in s3 bucket.? Questions around security of data and over the network data transfer etc {-ans in Cloud/aws_questions.txt
8. What is streaming ?
      continuous flow of data generated and processed in real-time or near real-time. This is in contrast to batch processing, where             data          is    collected, stored, and processed in large chunks at a later time. Streaming enables immediate insights and          actions based on the data       as it arrives
9. Explain Kinesis .. what is storage time of Kinesis Detailed explanation on kinesis .
      Kinesis Data Streams:
         Amazon Kinesis is a suite of services designed to handle real-time data streaming and processing. It allows you to collect,                process, and analyze streaming data so you can get timely insights and react quickly to new information. Here's a detailed             explanation of the components and capabilities of Amazon Kinesis:
         Purpose: Real-time data ingestion and processing.
         Features:
         Shards: The basic unit of scalability. Each shard can ingest up to 1 MB of data per second and 1,000 records per second. You                can increase or decrease the number of shards in your stream to accommodate your data throughput.
         Producers: Applications or devices that send data to a stream. Examples include web servers, mobile devices, sensors, and logs.
         Consumers: Applications that process the data from the stream. These could be AWS Lambda functions, EC2 instances, or Kinesis            Data Firehose.
         Data Retention: Data can be retained in the stream for 24 hours by default, extendable up to 7 days.
         Use Cases: Real-time log and event data collection, real-time analytics, and data replication.
         
      Kinesis Data Firehose:
         
         Purpose: Load streaming data into data stores and analytics services.
         Features:
         Delivery Destinations: Supports Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk.
         Data Transformation: Allows for data transformation using AWS Lambda before delivery.
         Automatic Scaling: Scales automatically to match the throughput of incoming data.
         Data Format Conversion: Can convert incoming data to formats such as JSON, CSV, or Parquet.
         Use Cases: Simplified ETL (Extract, Transform, Load) processes, real-time data analytics, and archiving.
      
      Kinesis Data Analytics:   
         Purpose: Real-time data processing and analysis using SQL.
         Features:
         SQL Queries: Enables you to write SQL queries to process and analyze streaming data.
         Integration: Can consume data from Kinesis Data Streams and Kinesis Data Firehose.
         Real-Time Metrics and Alarms: Provides real-time metrics and the ability to set alarms for specific conditions.
         Use Cases: Real-time analytics, monitoring, and alerting based on streaming data.
        
      Kinesis Video Streams:
         Purpose: Ingest, process, and store video streams for analytics and machine learning.
         Features:
         Live and On-Demand Video: Supports live streaming and on-demand playback of video.
            Integration with ML and Analytics: Can integrate with AWS services like Amazon Rekognition for video analysis, AWS Lambda             for real-time processing, and more.
         Data Retention: Configurable retention periods for video data.
         Use Cases: Security monitoring, live video broadcasting, machine learning inference, and IoT video applications.
10 . Explain Firehose ..difference between kinesis and firehose. see above
11. explain SQS Detailed
12 .Explain  SNS Detailed
13. Explain SES Detailed
14 .Comparison between Firehose and SQS ,why should i use firehose 
15.S3 life cycle  :- ans in Cloud/aws_questions.txt
16. What is a glue connection .
17. Inside the glue how do you do resource allocation :- from resouce allocation dropdown g1x,g2x....
18. explain in detail about G1x , G2x ,G3x and the number of DPU's in each etc
      In AWS Glue, the term "G" followed by a number refers to different worker types used for running ETL (Extract, Transform, Load)          jobs. Each worker type provides a specific amount of processing power, memory, and disk space. The concept of DPUs (Data                 Processing Units) is central to understanding the capacity and cost of running AWS Glue jobs. Here's a detailed explanation of the       worker types G1.x, G2.x, and G3.x, and their corresponding DPUs:
   
      AWS Glue Worker Types and DPUs
      G1.x Worker Type
         Purpose: Suitable for general ETL workloads.
         Compute Resources: Each G1.x worker provides 1 DPU.
         Memory: Approximately 4 GB of memory per DPU.
         vCPUs: Approximately 2 vCPUs per DPU.
         Disk Space: Approximately 50 GB of disk space per DPU.
      G2.x Worker Type
         Purpose: Designed for memory-intensive ETL workloads.
         Compute Resources: Each G2.x worker provides 1 DPU.
         Memory: Approximately 8 GB of memory per DPU.
         vCPUs: Approximately 4 vCPUs per DPU.
         Disk Space: Approximately 50 GB of disk space per DPU.
      G3.x Worker Type
         Purpose: Optimized for both compute and memory-intensive ETL workloads.
         Compute Resources: Each G3.x worker provides 1 DPU.
         Memory: Approximately 16 GB of memory per DPU.
         vCPUs: Approximately 4 vCPUs per DPU.
         Disk Space: Approximately 64 GB of disk space per DPU.
      In AWS Glue, one DPU (Data Processing Unit) consists of 4 vCPUs and 16 GB of memory. This definition applies to the overall                capacity of DPUs, irrespective of the specific worker types (G1.x, G2.x, G3.x) discussed earlier.
19. In glue connection ,how it connects securely  and what are the components.
    Throught , vpc ,subnet and security groups and jdbc ursl with passowrd stired in secerete managers
20.  Write a code for below scenario   :- Answer in python/code_assignments.txt
      we have 2 dfs with huge dataset with only one column , i want to generate a third df 
       with 3 columns col1 from first df , column 2 from second df and third column addition of  first and second dfs columns.
    once you finish the writing the code questions around the code 
21. What will the time complexity of the above code and its algorithm.
22. how is time complexity calculated 
23. What are the solid principals of Data engineering 
   The SOLID principles are fundamental design principles in software development that aim to make software designs more understandable, flexible, and maintainable. While they are typically associated with object-oriented programming, many of these principles can be applied to data engineering practices as well. Here's how each SOLID principle can be interpreted in the context of data engineering:
   
Single Responsibility Principle (SRP):
   
   Principle: A class (or module, component) should have only one reason to change.
   Application in Data Engineering: Data pipelines and components should be designed to handle a specific responsibility or task. For       example, a data ingestion module should focus solely on ingesting data without also handling transformations or loading operations.       This principle helps in maintaining clarity and reducing complexity in data workflows.
  
Open/Closed Principle (OCP):
   
   Principle: Software entities (classes, modules, functions) should be open for extension but closed for modification.
   Application in Data Engineering: Data processing workflows should be designed to allow for new functionality (extensions) to be added    without modifying existing code. This can be achieved through modularization and using configuration-driven approaches where             possible, enabling easier scalability and evolution of data pipelines.
  
Liskov Substitution Principle (LSP):
   
   Principle: Objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program.
   Application in Data Engineering: In the context of data structures or formats, adhering to consistent schemas and interfaces allows different data sources or formats to be seamlessly integrated or replaced within data pipelines. This ensures interoperability and avoids unexpected behavior during data processing.
   Interface Segregation Principle (ISP):
   
   Principle: Clients should not be forced to depend on interfaces they do not use. Instead of one fat interface, many small interfaces are preferred.
   Application in Data Engineering: Data processing components should expose specific interfaces tailored to their functionalities. For example, a data transformation module should expose methods or interfaces relevant to transformation operations, while a data loading module should expose methods for data loading tasks. This principle promotes modularity and reduces the impact of changes by minimizing dependencies.
   Dependency Inversion Principle (DIP):
   
   Principle: High-level modules should not depend on low-level modules. Both should depend on abstractions (interfaces). Abstractions should not depend on details. Details should depend on abstractions.
   Application in Data Engineering: Data engineering pipelines should rely on abstract interfaces or contracts rather than specific implementations. For instance, decoupling data processing logic from the underlying storage or compute infrastructure allows for easier migration to different platforms or technologies. This principle supports flexibility, scalability, and maintainability in data engineering architectures.
   Benefits of Applying SOLID Principles in Data Engineering
   Maintainability: Enhances the readability and understandability of data pipelines, making it easier to maintain and modify.
   Scalability: Facilitates the addition of new features or functionality without major changes to existing components.
   Flexibility: Promotes modular design, allowing components to be reused and replaced as needed.
   Testability: Encourages the development of components that are easier to test in isolation, improving overall system reliability.
   By applying SOLID principles in data engineering practices, teams can build robust, adaptable, and maintainable data pipelines and       systems that effectively meet business requirements and evolve with changing data needs.
24. what is ICEBERG  table ?what are the files in the ICEBERG  tables ? how does it works 
25. Write a code for below scenario 
    there is one array list with some elements from 0 to 10 find out the 3 elements whose addition is 6 
     write the detailed code.
26.Is python pass by value or pass by reference explain in details with example 
27.are Java and Scala pass by value or pass by reference explain in details, questions with example of coding and methods/list etc  
28. what are route tables and questions on networking 
29.What is delta lake ?
30 what is kafka
31 what is data mesh   
32 what is UDF , how to create a UDF , 
33 how is udf different to already existing spark functions and how does it work.   
34 what happens underneath when UDF runs and why it is slower .
35 pandas library      
36. some scenario-based questions
37. in SNS when you send a notification email can you send a table of data into it in the email or logs in the email.
38. How did you perform TYpe 1 and type 2 ?
39. Can we do type 1 and type2 on parquet files?
40. What is aws lake formation ?

               AWS Lake Formation is a managed service provided by Amazon Web Services (AWS) to simplify the process of setting up,             securing, and managing data lakes. A data lake is a centralized repository that allows you to store all your structured and             unstructured data at any scale. Lake Formation aims to make it easier to manage data lakes by automating many of the complex tasks       involved in their creation and management.
         
         Key Features of AWS Lake Formation
         Simplified Data Ingestion:
         
         Data Import: Simplifies the ingestion of data from various sources, such as databases, data streams, and object storage.
         Automated ETL: Automates Extract, Transform, Load (ETL) processes to move data into the data lake and prepare it for analysis.
         Centralized Security Management:
         
         Access Control: Centralizes access control policies for data stored in the data lake. It integrates with AWS Identity and                Access Management (IAM) and AWS Key Management Service (KMS) for fine-grained access control and encryption.
         
         Data Governance: Provides tools for setting and managing data governance policies, ensuring that data is properly secured and             compliant with organizational policies.
         
         Data Catalog and Discovery: 
         AWS Glue Data Catalog: Integrates with the AWS Glue Data Catalog to automatically catalog data, making it easier to discover,             understand, and manage.
         
         Metadata Management: Maintains metadata about the data stored in the lake, including schemas, tables, and partitions.
         Data Preparation and Transformation:
         
         Blueprints: Offers pre-defined blueprints for common ETL tasks, reducing the time and effort required to set up data workflows.
         Data Transformation: Supports data transformation using AWS Glue, enabling you to clean, enrich, and prepare data for analysis.
         Integration with Analytics and Machine Learning:
         
         Analytics Integration: Seamlessly integrates with AWS analytics services like Amazon Athena, Amazon Redshift, Amazon EMR, and             Amazon QuickSight for data analysis and visualization.
         Machine Learning: Supports integration with machine learning services like Amazon SageMaker for building and deploying ML                models on data stored in the data lake.

41. Permissions
      Permissions in AWS Lake Formation are defined to control access to data lake resources. They specify which users or roles can             perform certain actions on the resources.
      
      Types of Permissions
      Database and Table Permissions:
      
      Create: Permission to create databases and tables.
      Drop: Permission to delete databases and tables.
      Alter: Permission to modify the schema of databases and tables.
      Select: Permission to read data from tables.
      Insert: Permission to write data to tables.
      Delete: Permission to delete data from tables.
      Column-level Permissions:
      
      Fine-grained permissions to control access at the column level within tables.
      Location-based Permissions:
      
      Permissions to control access to specific data locations in Amazon S3.
      Tag-based Permissions:
      
      Permissions defined based on LF-tags. These allow setting access policies on groups of resources tagged with specific LF-tags.

42. LF-tags
         In AWS Lake Formation, LF-tags (Lake Formation tags) and permissions are mechanisms used to manage and control access to data          in a data lake. They provide a flexible and scalable way to implement fine-grained access control and data governance policies.
         
         
         LF-tags are key-value pairs that can be attached to Lake Formation resources such as databases, tables, columns, and data          locations. They are used to categorize and manage resources, and to implement access control policies based on tags rather than          specific resource names.

      Benefits of Using LF-tags and Permissions
      Simplified Management: Managing permissions via tags simplifies the process, especially in large data lakes with many resources.
      Scalability: Tag-based policies can easily scale as new resources are added. Simply tagging new resources automatically applies          the relevant policies.
      Flexibility: Allows for dynamic and flexible access control policies that can adapt to changing organizational needs.
      Centralized Governance: Provides a centralized way to manage and enforce data governance policies across the data lake.
