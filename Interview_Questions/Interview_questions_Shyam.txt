1.What is Partition and what is bucketing?

partition In Hive, tables are created as a directory on HDFS. 
A table can have one or more partitions that correspond to a sub-directory for each partition inside a table directory.

Hive Bucketing a.k.a (Clustering) is a technique to split the data into more manageable files,
(By specifying the number of buckets to create). The value of the bucketing column will be hashed by a user-defined
 number into buckets.Bucketing can be created on just one column, you can also create bucketing on a partitioned table 
to further split the data which further improves the query performance of the partitioned table.

Each bucket is stored as a file within the table‚Äôs directory or the partitions directories. 
Note that partition creates a directory and you can have a partition on one or more columns; 

set hive.exec.reducers.bytes.per.reducer=67108864; 

Partitioning and Bucketing in Hive

  Both Partitioning and Bucketing in Hive are used to improve performance 
  by eliminating table scans when dealing with a large set of data on a Hadoop file system (HDFS)
  
  Year partioned and month bucketing
  CREATE TABLE zipcodes(
    RecordNumber int,
    Country string,
    City string,
    Zipcode int)
    PARTITIONED BY(state string)
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ',';

   CREATE TABLE zipcodes(
     RecordNumber int,
     Country string,
     City string,
     Zipcode int)
     PARTITIONED BY(state string)
     CLUSTERED BY Zipcode INTO 10 BUCKETS
     ROW FORMAT DELIMITED
     FIELDS TERMINATED BY ',';

    PARTITIONING	                                      BUCKETING
    --------------------------------------------------------------------------------------------------------------------------------
    Directory is created on HDFS for each partition.	      File is created on HDFS for each bucket.
    You can have one or more Partition columns	              You can have only one Bucketing column
    You can‚Äôt manage the number of partitions to create	      You can manage the number of buckets to create by specifying the count
    NA	                                                      Bucketing can be created on a partitioned table
    Uses PARTITIONED BY	                                      Uses CLUSTERED BY
    do not create partitions on the columns with              Create buckeiting on high cardinality(more Unique Values) columns
    very high cardinality create on low cardinality.          If some map-side joins are involved in your queries, then bucketed tables are a good option. 
    For example- product IDs, timestamp,                      Map side join is a process where two tables are joins using the map function only without any reduced function
    and price because will create millions of directories 
    which will be impossible for the hive to manage
    We cannot do partitioning on a column with very high cardinality. 
    Too many partitions will result in multiple Hadoop files 
   which will increase the load on the same node 
   as it has to carry the metadata of each of the partitions
https://www.analyticsvidhya.com/blog/2020/11/data-engineering-for-beginners-partitioning-vs-bucketing-in-apache-hive/

 hive.mapjoin.smalltable.filesize=(default it will be 25MB)

2.What is Persist() and Cache()
Spark Cache and persist are optimization techniques for iterative and interactive Spark applications to improve the performance of the jobs or applications
Though Spark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations 
you will see degrade in performance when you are dealing with billions or trillions of data.Hence, we may need to look at the 
stages and use optimization techniques as one of the ways to improve performance.


Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate 
computation of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, 
Dataframe, and Dataset computation result‚Äôs).

But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to 
store it to the user-defined storage level.

When you persist a dataset, each node stores its partitioned data in memory and reuses them in 
other actions on that dataset. And Spark‚Äôs persisted data on nodes 
are fault-tolerant meaning if any partition of a Dataset is lost, 
it will automatically be recomputed using the original transformations that created it

Cost efficient ‚Äì Spark computations are very expensive hence reusing the computations are used to save cost.

Time efficient ‚Äì Reusing the repeated computations saves lots of time.

Execution time ‚Äì Saves execution time of the job and we can perform more jobs on the same cluster.

3.What is coalesce?
https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/

homerDf = numbersDf.repartition(2)
rdd1 = rdd.coalesce(4)
The repartition method does a full shuffle of the data, so the number of partitions can be increased

A full data shuffle is an expensive operation for large data sets, but our data puddle is only 2,000 rows. 
The repartition method returns equal sized text files, which are more efficient for downstream consumers

4.What is groupBykey vs Reduceby key

 Reduceby key aggrigate and then shuffle where as group by will shuffle all values according to key pair in another rdd this leads lot of data transfer
 Both reduceByKey and groupByKey result in wide transformations which means both triggers a shuffle operation.
 The key difference between reduceByKey and groupByKey is that reduceByKey does a map side combine and groupByKey does not do a map side combine.

5.Fill null values
   #Replace 0 for null for all integer columns
   df.na.fill(value=0).show()

   #Replace Replace 0 for null on only population column 
   df.na.fill(value=0,subset=["population"]).show()
  
  #String columns
   df.na.fill("unknown",["city"]) \
    .na.fill("",["State"])

6. Vectorization
   Vectorized query execution is a Hive feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. 
   A standard query execution system processes one row at a time.
   Vectorized query execution streamlines operations by processing a block of 1024 rows at a time.
   
   set hive.vectorized.execution.enabled = true;

7. Distinct rows or duplicates
   df.dropDuplicates()
   df.distinct()

8.leftanti join returns only columns from the left dataset for non-matched records
  leftsemi join returns columns from the only left dataset for the records match in the right dataset on join expression,
  records not matched on join expression are ignored from both left and right datasets

9. map() ‚Äì Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.
flatMap() ‚Äì Spark flatMap() transformation flattens the DataFrame/Dataset after applying the function on every element and returns a new transformed Dataset. 
The returned Dataset will return more rows than the current DataFrame.

10.SET hive.exce.parallel=true;
   SET hive.vectorized.execution.enabled=true
   SET hive.exec.dynamic.partition.mode = nonstrict
   SET hive.enforce.bucketing=true
 
  ANALYZE TABLE test1 COMPUTE STATISTICS;
  set hive.stats.autogather=false;

 SET spark.sql.sources.bucketing.enabled=true;


11. Skip N Rows
    df.withColumn("Index",monotonically_increasing_id())
        .filter(col("Index") > 2)
        .drop("Index")

  from pyspark.sql import functions as F 
  expr = [F.last(col).alias(col) for col in df.columns]
    df.agg(*expr).show()

  Last TO N Rows 
  from pyspark.sql.functions import monotonically_increasing_id  
  from pyspark.sql.functions import desc
   
  df_cars = df_cars.withColumn("index", monotonically_increasing_id())
  df_cars.orderBy(desc("index")).drop("index").show(5)


12. Yarn logs in Cloudera stores in ===> var/log/hadoop-yarn --check with dev env

13. Check point -- last modified and chekpoint acts as a delta point 
14. Maximum file size can be loaded in hive

   hive.exec.reducers.bytes.per.reduce   
   Size per reducer. The default in Hive 0.14.0 and earlier is 1 GB, that is, if the input size is 10 GB then 10 reducers will be used. 
   In Hive 0.14.0 and later the default is 256 MB, that is, if the input size is 1 GB then 4 reducers will be used.
   hive.target-max-file-size

  MapR-FS chunk size
Files in MapR-FS are split into chunks (similar to Hadoop blocks) that are normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size.
The actual split size is max(target split size, chunk size).

Best effort maximum size of new files. 1GB

 A checkpoint is the last load of saved data. It captures FsImage and editlogs the namespace log, then compacts both into a new FsImage. 
 This is a continuous process. This task of creating a checkpoint is performed by Secondary NameNode.


15.Parallel execution

> SET hive.exec.parallel=true; -- default false
> SET hive.exec.parallel.thread.number=16; - -- default 8

16. Sort() and OrderBy()
  
sort() method will sort the records in each partition and then return the final output which means that the order of the output data is not guaranteed because 
the data is ordered on partition-level but your DataFrame may have thousands of partitions distributed across the cluster. 
Since the data is not collected into a single executor

orderBy() function guarantees a total order in the output. This happens because the data will be collected into a single executor in order to be sorted.
   This means that orderBy() is more inefficient compared to sort()

  df.orderBy('country', 'age').show(truncate=False)

17. Second Nth Highest Salary

    select * from( select ename, sal, dense_rank() over(order by sal desc)r from Employee) where r=&n

20.  row_number(): Returns a sequential number starting from 1 within a window partition

    rank() : Returns the rank of rows within a window partition, with gaps.
   
    percent_rank(): Returns the percentile rank of rows within a window partition.

   dense_rank(): Returns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.

   from pyspark.sql.functions import rank
   df.withColumn("rank",rank().over(windowSpec)) \
    .show()

  monotonically_increasing_id() is unique but not consecutive if the dataframe has more than 1 partition.
  The exchange of data happens using RoundRobinPartitioning technique which takes care of 
  even distribution of data across partition. We can also conclude from above point that this method assumes that 
  a partition will not have more than 8589934592 records i.e. ~8 Bn records in a single partition

21. Leftjoin and leftanti join

  leftjoin gives all the rows from left table

  leftanti join gives records which are not in right table matching records will be ignored.

22. To read nested directory files
    val df= sparkSession.read
       .option("recursiveFileLookup","true")
      .option("header","true")
      .csv("src/main/resources/nested")


23. Name Node,Secondary and Stand by node
    In Hadoop 3.x you can have two passive NameNodes along with the active node, 
    as well as five JournalNodes to assist with recovery from catastrophic failures

    >NameNode only stores the metadata of HDFS ‚Äì the directory tree of all files in the file system, and tracks the files across the cluster.
    >Secondary name is not aback up of Name node, Secondary name will take snapshot and keep meta data and keeps edit logs upto date.
    It will not take control and not act as Name node. 
    >Standby name node will take backup periodically if name node failures then standby node act as a active name node
     So a Standby Node comes in action which is nothing but a backup for the Name Node
    >main difference between Secondary and standby namenode is secondary namenode does not upload the merged Fsimage with editlogs to active namenode where 
     as the standby node uplods the merged new image back to active Namenode. 
     So the NameNode need to fetch the state from the Secondary NameNode
    >Node Manager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster

https://www.projectpro.io/article/getting-to-know-hadoop-3-0-features-and-enhancements/354
https://bigdatapath.wordpress.com/2020/07/17/hadoop-2-x-vs-hadoop-3-x/

24. Hive Components
    The major components of Apache Hive are the Hive clients, Hive services, Processing framework and Resource Management, and the Distributed Storage.

25..option('mode','DROPMALFORMED ')

set spark.sql.csv.parser.columnPruning.enabled to false

26.Spark 2.x versions have support for head() function which will give the first few rows of the dataframe. 
In Spark 3.0 version tail() function has been introduced which will print the last few rows of the DF.

Spark 3.0 was released on 16 June 2020
Programming language and Platform Support:
*Complete support for JDK 11
*Scala version updated to 2.12
*Python version 2.x is deprecated.
*Support for Hadoop 3

28.alter table table_name SET TBLPROPERTIES('EXTERNAL'='TRUE');
While creating table 
 TBLPROPERTIES ('external.table.purge'='true');   will also delete external table

Spark Transaformations
 read(),select(),groupby(),filter(),join(),union(),orderby()
Spark actions()
 take(),colllect(),show(),count(),save()
 

Python
----------------
1.Loops
2.Group by aggrigation customer name and id and order top5 orders
3.Extra column with subject
4.apply lambda, lambda,filter,map
5.iloc and loc
  The loc() function is label based data selecting method
  # selecting range of rows from 2 to 5
    display(data.loc[2: 5])

  # selecting 0th, 2th, 4th, and 7th index rows
    display(data.iloc[[0, 2, 4, 7]])

6.two arrays multiple np.product
7.List null values will not be allowed
8.any function retruns tuple value
9.dictionary a,b
10.Transamations null vlaues,min max scalar > min 0 max 1 when all variables has equal importance 
  and standard scalar x-U/sd 
11.date time conversion, float ciel and floor, mod, Import math
12.7%2 remainder value
13.list and dictionary compre
14.generators,oops

 Parallisum Technique
---------------
https://medium.com/@vinu_learn/spark-performance-optimization-techniques-5dd46a68db13

https://nivedita-mondal.medium.com/spark-interview-guide-part-4-important-concepts-22a464b693d0

https://nivedita-mondal.medium.com/the-small-files-problem-in-hadoop-spark-9e7342fbdecf

1. What is the file size hive can handle?
2. Memory of 1 Gb but file size recieved is 3 gb how to handle in Spark? (1 Tb data and cluster is small) 
    Similar question loading a file of bigger size how to handle
    use persist method to store data in memory_and_disk
	--chuncks concept of python
3. While loading a file how to remove duplicates?
4. Highest order function in hive?
5. Initially Spark job was running fine when there is a data of 3 gb later data size increased to 5 gb business dont want 
  to create a CR as earlier job was success how to handle?
6. Modes in streaming?
7. Spark Job is running one of the task takes long time how to solve it?
8. In hive hql file how to make queries to run in parallel?


--resource allocation 
  10 Nodes and each node have 16 cores, 64 GB RAM
  yarn allocati core =10*15 = 150 cores
  memory = 10*63 =630 GB
  
  executors = 150 / 5 = 30 -1 =29
           30/10 =3 execuors per node
  memory= 63/3 =21 _2gb overhead  = 19 gb

Zip with Index counter will be in all nodes where Monotically increating id will be in one Node


int8 can store integers from -128 to 127.
int16 can store integers from -32768 to 32767.
int64 can store integers from -9223372036854775808 to 9223372036854775807.

Transpose Rows to Column Or Convert Rows to Columns
-------------------------------------------------
SELECT    
    max(DECODE(color, 'Red', qty)) AS "Red",
    max(DECODE(color, 'Green', qty)) AS "Green",
    max(DECODE(color, 'Black', qty)) AS "Black" 
   FROM colors;


SELECT    
    max(case when color, 'Red' then  qty end)) AS "Red",
    max(case when color, 'Green' then qty end)) AS "Green",
    max(case when color, 'Black' then qty end)) AS "Black" 
   FROM colors;

-- PIVOT FUNCTION
SELECT * FROM (
  SELECT NAME,CITY
  FROM USA_HOSPITALS)
PIVOT(COUNT(CITY) 
  FOR (CITY) IN ('MINOT', 'SITKA', 'ANCHORAGE'));

-- DECODE FUNCTION
SELECT
    Name,
    COUNT(DECODE(CITY, 'MINOT', CITY)) AS "MINOT",
    COUNT(DECODE(CITY, 'SITKA', CITY)) AS "SITKA",
    COUNT(DECODE(CITY, 'ANCHORAGE', CITY)) AS "ANCHORAGE" 
   FROM USA_HOSPITALS


Handle Special Characters
---------------------
df['column_name'].str.encode('ascii', 'ignore').str.decode('ascii')

spark = SparkSession.builder \
    .appName(app_name) \
    .master(master) \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

data = [[1, 'ABCDEDF!@#$%%^123456qwerty'],
        [2, 'ABCDE!!!']
        ]

df = spark.createDataFrame(data, ['id', 'str'])
df.show(truncate=False)
df = df.select("id", regexp_replace("str", "[^0-9a-zA-Z_\-]+", ""
                                    ).alias('replaced_str'))

incremental
----------------------
sqoop import --connect jdbc:teradata://{host name}/Database=retail 
--connection-manager org.apache.sqoop.teradata.TeradataConnManager 
--username dbc --password dbc --table SOURCE_TBL --target-dir /user/hive/incremental_table -m 1 
--check-column modified_date --incremental lastmodified --last-value {last_import_date}

https://dwgeek.com/hive-incremental-load-options-and-examples.html/


Hive Performance
-------------------
Partitions
Buckting
Avoid Locking
Use Tez Engine
Vectorization
and Parallel enable
Use STREAMTABLE option
Map Side Join

https://www.analyticsvidhya.com/blog/2022/02/performance-tuning-practices-in-hive/

Kill Process
----------------
ps -aux | kill -9 {pid}

Grep Command
------------
ps -aux | grep '**serachstring*'


Cloudera Version --> 7.1.7
Hadoop --> 3.1.1.7.1.7(hadoop version)
Hive Vesrsion --> 3.1.3(hive --version)
Sqoop --> 1.4.7.1.7(sqoop version)
Jdk --> 1.8.0_32(Java -version)
Spark--> 2.4.7.7.1.7(spark--shell)
Scala--> 2.11.12(spark--shell)

https://devopsrecipe.wordpress.com/2018/01/19/spark-2-x-benefits-over-spark-1-x/

Infer Schema
inferSchema -> Infer schema will automatically guess the data types for each field. If we set this option to TRUE, 
the API will read some sample records from the file to infer the schema. If we want to set this value to false, we must specify a schema explicitly

#predefined schema of StructType 
sch=StructType([
  StructField("2019_rank",IntegerType(),True),
  StructField("City",StringType(),True),
  StructField("State_Code",StringType(),True),
  StructField("2019_estimate",IntegerType(),True),
  StructField("2010_Census",IntegerType(),True),
  StructField("Change",StringType(),True),
])

#read the data with defined schema
df = spark.read \
  .option("header", True) \
  .option("delimiter", "|") \
  .schema(sch) \
  .csv(file_location)

cus_count = customers.where(customers.state == 'TX').groupBy('state', 'city').agg(count('id').alias('cus_count')).orderBy('cus_count', ascending=False).limit(25)

result = cus_count.withColumn('rank', rank().over(window)).withColumn('dense_rank', dense_rank().over(window)).withColumn('row_num', row_number().over(window))

Or

result1=empDF.groupBy("emp_dept_id","name")\
     .agg(max("salary").alias("salary")).withColumn('Rank',dense_rank().over(Window.orderBy(col("salary").desc()))).select('Rank',"salary","emp_dept_id","name").show()

result1=empDF.groupBy("emp_dept_id","name")\
     .agg(max("salary").alias("salary")).withColumn('Rank',dense_rank().over(Window.orderBy(col("salary").desc()))).show()

here select is optional

Lag and Lead
----------
https://www.youtube.com/watch?v=n40Ub8t8GHw

Lag returns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.

Lead returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row.


from pyspark.sql.functions import *
from pyspark.sql.window import Window
windowSpec  = Window.partitionBy("department").orderBy(col("salary").desc())
Or
windowSpec  = Window.partitionBy("department").orderBy(col("salary"))
df.withColumn("lag",lag("salary",1).over(windowSpec)).withColumn("lead",lead("salary",1).over(windowSpec)).show()

windowSpec  = Window.orderBy(col("salary").desc())
empDF.withColumn('Rank',dense_rank().over(windowSpec)).show()

explode

df1=df.select(df.name,explode(subject))

Parallesium in Pyspark
---------------
Partitioning simply means dividing a large data set into smaller chunks(partitions).
 In Spark they are the basic units of parallelism and it allows you to control where data is stored as you write it.

Hive :
üî∏A single file represents one bucket.
üî∏Buckets are sorted.
üî∏Reducer operation populates a bucket (one reducer for one bucket), which requires sort/shuffle
üî∏Exact number of buckets are created as per the table definition

Spark:
üîπA collection of files comprises of one bucket.
üîπBuckets are not sorted.
üîπ Multiple files can be associated with a bucket, and writes doesnt require shuffle.

The following are a few transformation types;

     Map (func) - it returns a new disperse dataset build by passing each element of the source via a function called func.
     Filter (func) - it returns a new dataset developed by choosing those elements of the source on which func returns true.
     flatMap (func) ‚Äì it is equal to map, but each input item can be mapped to 0 or more resulting items (so func should return a Seq instead of a single item).
    mapPartitions (func) ‚Äì Equal to map, but runs individually on each block of the RDD, so the func must be of ‚ÄúIterator<T> => Iterator<U>‚Äù type while running on an RDD of type ‚ÄúT‚Äù.
   Sample (withReplacement, fraction, seed)  -Sample a fraction ‚Äúfraction‚Äù of the data, with or without replacement, with a given random number generator seed.
    Union(otherDataset)- it returns the latest dataset that includes the union of the elements within the source dataset and the argument.
   intersection(otherDataset)- this returns a new RDD that includes the intersection of elements under the source dataset and the argument.
    distinct([numTasks]))- Returns a latest dataset that includes the distinct elements of the dataset source.

Actions

The ‚ÄúActions‚Äù are generally useful to send output back to the driver. And hence they develop a non-distributed dataset.

  reduce(func) ‚Äì gather the elements of the dataset with the use of a function func (it takes two arguments and returns a single one). The function should be commutative and associative to compute correctly in parallel.
  collect() ‚Äì Returns all the elements within the dataset as an array at the driver program. This is generally useful after a filter or other operation that returns enough small subset of the data.
   count() ‚Äì it returns the number of elements that exist in the dataset.
   first() ‚Äì it returns the first or starting element of the dataset (similar to take(1)).
   take(n) ‚Äì it returns an array with the starting ‚Äún‚Äù elements within the dataset.

Parallelsim in Spark
--------------------
RDD
Concgcthreads
parallegcthread
ThreadPool
native libraries
UDF

from multiprocessing.pool import ThreadPool
pool = ThreadPool(10)


def fun(x):
    try:
        df = sqlContext.createDataFrame([(1,2, x), (2,5, "b"), (5,6, "c"), (8,19, "d")], ("st","end", "ani"))
        df.show()
    except Exception as e:
        print(e)

pool.map( fun,data)

What is your cluster size?

40 Node cluster with each of 7 TB with 16 cores and with a memory of 128 GB

collect() and count()


The method count sums the number of entries of the RDD for every partition, and it returns an integer consisting in that number, 
hence the data transfer is minimal; in the other hand, the method collect as its name says brings all the entries 
to the driver as a List therefore if there isn't enough memory you may get several exceptions 
(this is why it's not recommended to do a collect if you aren't sure that it will fit in your driver,
there are normally faster alternatives like take, which also triggers lazy transformations), 
besides it requires to transfer much more data.

How to ensure copiying SFTP file only after fully copied into server?

Star Schema Vs Snoflake Schema?

Pyspark Partition
------------
empDf.write.option("header",True).option("maxRecordsPerFile",2).partitionBy("emp_dept_id").csv(r'C:\Users\Vishnu\Documents\Shyamcsv1')
https://www.geeksforgeeks.org/pyspark-partitionby-method/

Hive uses the Hive hash function to create the buckets where as the Spark uses the Murmur3

Sql Query
--------------
Start_date 01-Jan-2022 End_date 31-Dec-2022
Need output of each month start and end date
https://blogs.oracle.com/sql/post/how-to-generate-days-weeks-or-months-between-two-dates-in-oracle-database

select start_date,last_day(start_date) last_date from(select  add_months (   
           date'2022-01-01',  
           level-1
         ) as start_date  
  from   dual  
  connect by level <= months_between (  
    date'2022-12-31',  
    date'2022-01-01')+1);

Col1 col2 col3 col4 result 
------------------------------
1    1               2
           1         1
1    1     1    1    4  

SELECT   Column1,
         Column2,
         Column3,
         Column4,
         CASE WHEN Column1 IS NOT NULL THEN 1 ELSE 0 END + 
         CASE WHEN Column2 IS NOT NULL THEN 1 ELSE 0 END + 
         CASE WHEN Column3 IS NOT NULL THEN 1 ELSE 0 END + 
         CASE WHEN Column4 IS NOT NULL THEN 1 ELSE 0 END AS Column5
FROM     Table

select col1,col2,col3,
(case when col1=0 then 0 else 1 end +
case when col2=0 then 0 else 1 end +
case when col3=0 then 0 else 1 end ) col4
from demo1;
  



Read Multiple Tables
-------------------------------------
table_list = ['table1', 'table2','table3', 'table4']

for table in table_list:
     jdbcDF = spark.read \
         .format("jdbc") \
         .option("url", "jdbc:postgresql:dbserver") \
         .option("dbtable", "schema.{}".format(table)) \
         .option("user", "username") \
         .option("password", "password") \
         .load()

Useful Link
-------------------
https://techvidvan.com/tutorials/hadoop-architecture/

https://data-flair.training/blogs/hadoop-hdfs-architecture/


SALT is a technique that adds random values to push Spark partition data evenly. 
It's usually good to adopt for wide transformation requires shuffling like join operation